Medical imaging systems for tumor detection in MRI scans face significant challenges due to the complexity of identifying subtle and irregular anomalies. Traditional diagnostic approaches often struggle to produce accurate and reliable reports, particularly in cases involving intricate anatomical variations or ambiguous tumor boundaries. This research introduces an innovative framework that generates comprehensive human-readable reports by integrating both visual and textual insights, thereby improving detection accuracy and supporting clinical decision-making.

The proposed model emphasizes critical regions of MRI scans to accurately delineate subtle and diffuse tumor boundaries while preserving essential features. A Convolutional Block Attention Module (CBAM) is employed to enhance feature representation, highlighting key regions and improving segmentation performance. Deep Supervision facilitates multi-scale learning, enabling the preservation of fine details and robustness in tumor boundary detection.

To enhance interpretability, the system combines enriched visual features with textual modalities using Large Language Models (LLMs), such as LLaMA, to produce clear, clinically meaningful reports. This approach effectively manages heterogeneous tumor appearances, addressing the generalization limitations of conventional segmentation methods. Overall, the framework advances MRI-based tumor detection by refining feature representation, improving segmentation accuracy in challenging regions, and delivering interpretable, LLM-driven reports to support clinical workflows.